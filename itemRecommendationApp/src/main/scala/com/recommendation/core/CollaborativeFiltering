package com.recommendation.core

import org.apache.spark.mllib.linalg.distributed.{ MatrixEntry, CoordinateMatrix }
import org.apache.spark.{ SparkConf, SparkContext }
import org.apache.spark.SparkContext._

case class BuyerAffinity(userId:Long, itemId:Long, affinity:Double)

object CollaborativeFiltering {

  def main(args: Array[String]) {
	val sc = new SparkContext (new SparkConf ().setAppName ("Collaborative Filtering(User-Item"))

    val data = sc.textFile("file:///home/pchoudhary/fraudRisk/testData.txt")

    val userToItem = data.map(_.split("\t").toSeq)

    val itemToUserAffinity = userToItem.map(x => (x(1),x(0))).map(y => (y,1)).
      reduceByKey(_ + _).map(result => BuyerAffinity(result._1._1.toLong, result._1._2.toLong, result._2))

    val userItemMatrix = new CoordinateMatrix(itemToUserAffinity.map {
      case BuyerAffinity(userId, itemId, affinity) => MatrixEntry(userId, itemId, affinity)
    })

    val itemSimilarities = userItemMatrix.toRowMatrix.columnSimilarities

    val result = itemSimilarities.entries.map {
      case MatrixEntry(item1, item2, cosineSimilarity) => ((item1, item2), cosineSimilarity)
    }

    // Display the results
    result.foreach(x => println(x._1 + "\t" + x._2))
  }
}